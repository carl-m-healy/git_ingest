Thank you for the detailed clarification. I’ll begin a deep code review of your `git_ingest` repo with the goal of removing all REST functionality, streamlining program flow with modular and pure functions, and maximizing GitHub GraphQL data extraction for repos, branches, PRs, and commits under strict query and timeout constraints. I’ll propose a detailed, step-by-step development plan that an AI coding agent can use to iteratively implement these improvements.

I’ll let you know as soon as the plan is ready for review.


# GitHub Ingestion Tool Refactoring Plan

## Removing REST API Dependencies

Currently the code supports both REST (v3) and GraphQL (v4) GitHub APIs, with a `--graphql` flag toggling behavior. The first step is to eliminate all REST API calls and use GraphQL exclusively for data retrieval. This involves:

* **Removing REST calls and fallback logic:** Functions like `fetch_repos`, `fetch_branches`, `fetch_branches_full`, and `fetch_tags_full` rely on `requests.get` to REST endpoints (e.g. `GET /orgs/{org}/repos`, `/repos/{owner}/{repo}/branches`). These will be removed or no longer invoked. The CLI flow (in `cli()` and main logic) should be simplified to drop the `if args.graphql` conditional and always execute the GraphQL path. This means the `--graphql` flag can be deprecated or repurposed (since GraphQL will be the only mode). We will enforce requiring a token for all runs (GraphQL API requires authentication), so calling the tool without `--token` or `GITHUB_TOKEN` will produce an error (as it already does when `--graphql` was specified).

* **Using GraphQL for all data**: Ensure every piece of data previously gathered via REST is covered by GraphQL queries. The repository and branch listing is already implemented via GraphQL in `list_repos_branches_graphql`, and full repository/branch/tag details via `fetch_repos_full_graphql`. We will use these exclusively. Any remaining data that was fetched via REST (e.g. commit message bodies for branches/tags) should be included in the GraphQL queries so no information is lost. For example, the REST branch detail API provided the full commit message and commit metadata, which the current GraphQL query partially captures (only the `messageHeadline` – the commit title). We should extend the GraphQL commit fragment to include the full commit message or body if needed for parity. Likewise, the GraphQL `refs` query for tags currently assumes the ref’s target is a commit; we should add a fragment to handle annotated Git tags (e.g. `... on Tag { target { ... on Commit { ... } } }`) so that tags pointing to tag objects still yield the underlying commit data. With these adjustments, GraphQL will provide all repository info, branch names and commit metadata, and tag commit data that the REST calls covered.

* **Cleaning configuration and docs:** Remove references to REST page size or REST usage. For instance, the `GITHUB_REST_PAGE_SIZE` environment variable and related code can be deleted. Only `GITHUB_GRAPHQL_PAGE_SIZE` will remain to control pagination size. Update the README and CLI help to reflect that GraphQL is always used (the `--graphql` flag and any mention of REST can be removed). The user should be instructed that a token is required for operation (since unauthenticated GraphQL calls are not supported, this is a trade-off for simplifying the tool).

**Validation:** After removing REST pathways, run the test suite and update tests that assumed REST behavior. For example, `test_list_repos_branches` (which mocked `fetch_repos`/`fetch_branches`) should be removed or replaced with GraphQL equivalents. We can add a test to simulate no repositories found via GraphQL (monkeypatch `_github_graphql` to return an empty list) and verify the program prints a “No repositories found” warning (to maintain similar UX as before). Additionally, to ensure no data loss, we should run the tool on a sample user/org with both the old (REST) version and the new version, and compare outputs (branch lists, sample branch JSON, tag JSON) to confirm they contain the same information (aside from expected format changes, e.g. commit message fields).

## Streamlining Program Flow and Modularity

With the REST code removed, the program flow can be greatly simplified and made more modular:

* **Unifying code paths:** The dual implementation (REST vs GraphQL) led to duplicated logic (e.g. two ways to gather branches and tags). Now we can consolidate. The main logic in `cli()` can drop the large `if args.graphql ... else ...` split and instead always use the GraphQL functions. For example, we may call `fetch_repos_full_graphql` when the user requests full JSON output or tag details, and call a lighter function (or the same function with flags) when only branch names are needed. In fact, we can merge `list_repos_branches_graphql` and `fetch_repos_full_graphql` functionality by parameterizing the level of detail. One approach is to have a single GraphQL routine that by default fetches minimal info (repo names and branch names), and if a “full detail” flag is set, it fetches the extended fields (commits, PR counts, etc.) and tags. This reduces redundancy and ensures consistent data handling. The code already uses a boolean `need_full` to decide between these paths; we can make that more explicit in function calls rather than branching in the CLI logic.

* **Modularizing components:** We should refactor the monolithic `query_github.py` into logical units or at least logical sections with clear, pure functions. For example, the GraphQL query construction and pagination logic can be isolated in helper functions or even a separate module (e.g. a `github_queries.py`). The output persistence functions (`persist_branches`, `persist_repo_json`, etc.) are already nicely separated; those can remain as utility functions, possibly moved to their own module (`output.py` or similar) for clarity. This separation makes it easier to test each piece in isolation. We should ensure all functions that gather data (`fetch_repos_full_graphql`, etc.) return data structures and do not themselves print or write to disk (currently they do not, which is good). Any user feedback (like warnings or logging for skipping a repository) can be handled at the CLI level so that the core functions stay “pure” in terms of just computing results. For instance, in the REST version `list_repos_branches` printed a warning when skipping a repo; in the new GraphQL flow, similar error handling (e.g. if one repo’s branch query fails) could be done by catching exceptions around the GraphQL calls and logging/skipping, rather than aborting everything. This will make the tool more robust and the functions easier to reuse (or test via monkeypatching) without side effects.

* **Removing redundancy:** Any leftover variables or abstractions that are no longer needed should be removed. For example, the `_apply_gql_page_size` helper (which replaces `"first: 100"` in query strings) appears unused – we can delete it to reduce clutter. The global `API_BASE` and `_github_get` logic will no longer be needed once REST is gone, so those can be removed, along with the global `API_CALL_COUNT` increment in `_github_get`. We will retain `_github_graphql` (for GraphQL POST with retry) but may rename it for clarity (e.g. `_github_request`) since it will be the primary request function. The `API_CALL_COUNT` can still be maintained for GraphQL calls if desired (to log how many calls were made), or we can reset it to 0 at start and count GraphQL calls only. By cleaning up these remnants, the code becomes easier to follow.

* **Consistency in output logic:** We should ensure features like `--save-dir` (saving branch name lists) and `--json` output still work seamlessly in the new flow. In the current GraphQL branch, if `--save-dir` was used without full JSON, the code path did **not** call `persist_branches` (a minor bug – it only calls `persist_branches` inside the full-detail branch). We will fix this by calling `persist_branches(data, save_dir)` after getting the branch list even in the simple listing scenario. After refactoring, there will be a single place where file outputs are handled, iterating over the results from GraphQL. All these output functions (for repo JSON, branch JSON, etc.) will receive data in the same format as before (for compatibility). The data model in the output can remain largely the same (e.g. each branch JSON has `commit` (sha + message) and `commit_full` with detailed commit info) – this format was already mimicked in the GraphQL path to resemble REST results. We might refine it (for example, if we include the full commit message in `commit_full`, we should include it in the `commit` summary or ensure the naming still makes sense). Any such adjustments should be clearly documented and covered by tests (e.g. ensure that `persist_repo_json` still writes the JSON files with expected keys).

**Validation:** After restructuring, run all unit tests to ensure they pass. We will update tests that depended on the REST functions. New unit tests will focus on the GraphQL functions. For instance, we can test `_paginate_refs_graphql` by simulating multiple pages of data (monkeypatching `_github_graphql` to return a first page with `hasNextPage=True` and a second page). We will also test the CLI at a high level: use `capsys` in a pytest to capture output of `cli()` given certain arguments (by monkeypatching `_github_graphql` with predefined data) – this can verify that `--json` outputs valid JSON, that `--save-dir` writes files, etc., all using the GraphQL path. These tests ensure our refactored flow produces the same user-visible results. Code quality improvements (like smaller, focused functions) will be validated by the ease of writing these tests for each piece.

## Optimizing GraphQL Queries for Performance

With GraphQL as the sole data source, we need to maximize its efficiency given the constraints: each query must complete in <10 seconds (due to a proxy timeout) and we have a hard cap of 5000 API calls per hour. Our refactoring will include specific query optimizations:

* **Batch data retrieval in a single query:** One reason to use GraphQL is to fetch related data in one round-trip. The current implementation already does this by querying repositories and their first batch of branches (and tags) together. We will continue this strategy, making sure the GraphQL query grabs as much as is reasonable in one go. For example, we should request a larger page size for repositories and branch lists. Currently the code defaults to a very conservative page size of 10 (set via `GITHUB_GRAPHQL_PAGE_SIZE=10` by default) to avoid timeouts. We can likely increase this to improve throughput. GitHub’s GraphQL allows up to 100 nodes per page, and the original code even commented that 50 was the default. We should experimentally determine a safe page size (for example, 50 or 100) that stays under 10 seconds for typical large responses. As a starting point, we might set the default to 50 and allow tuning via the env var. That means each GraphQL call would fetch 50 repositories and for each repository up to 50 branches (and 50 tags) in that single response. This will drastically reduce the number of pagination calls needed. For instance, listing 500 repositories with 50-per-page would take \~10 calls instead of 50 calls at 10-per-page – a 5× reduction in API calls. Similarly, branches within a repo will paginate less frequently. We must verify that, for an organization with very many branches in a single repo, retrieving 50 (or 100) branch refs with their commit info doesn’t exceed the time limit. We might implement a safeguard or configuration: e.g., allow an environment override to lower the page size if the user knows their environment’s proxy is strict. In testing, we should time a GraphQL query on a repository with many branches to ensure the default is safe.

* **Efficient pagination and batching:** Even with larger pages, some repos or orgs will require multiple calls. Our goal is to minimize the number of separate HTTP requests. The refactored `_paginate_refs_graphql` already loops to get all pages of branches or tags for a given repo in succession. We can enhance this by **batching multiple pagination requests in one query** when possible. For example, if after the initial query we find several repositories still have `hasNextPage=True` for branches, we could craft a single GraphQL query that requests the next page of branches for each of those repositories in parallel. This is possible by using multiple query aliases in one call (e.g., `repo1_branches: repository(owner:$org, name:"Repo1") { refs(... after: $cursor1) { ... } }` and similarly for repo2, etc., in one payload). Doing so requires dynamic query construction, but it could significantly cut down API calls when many repos have long branch lists. We should weigh this complexity: if most repos have few branches and only a handful have many, the current approach (one call per repo for extra pages) might be acceptable under 5000 calls. For truly large cases, though, implementing this batching will help stay within rate limits. We can plan this as an optional optimization step – ensuring correctness first with simpler logic, then improving it. If implemented, it will need careful testing (simulate multiple repos with paginated branches and verify that our combined query returns and merges data correctly).

* **Selective field fetching:** GraphQL performance is also impacted by the amount of data requested. We should review the fields we fetch for necessity. The current query for full repo details is quite comprehensive – it retrieves repository metadata (description, sizes, counts, settings) and even some relational data like first 10 collaborators and topics. While this provides a rich dataset, it can slow down the response. We should consider dropping or limiting non-essential fields to speed up queries. For example, if we only care about counts of watchers or languages, we might not need to fetch the actual language list edges or collaborator details (which involve extra nested objects). Each additional nested list (e.g. `repositoryTopics`, `collaborators`) adds to query processing time. We might decide to remove or further limit these to the bare essentials needed for the ingestion use-case. This will keep the query lightweight. Similarly, for branch data, we currently fetch up to 5 associated pull requests and check suite statuses. If these are not crucial for the intended analytics, we could omit them or provide an option to skip them, which would reduce query complexity. However, if they are deemed useful (e.g., to know if a branch has an open PR or CI status), we can keep them but remain mindful that each adds overhead. In summary, we will **trim the GraphQL query to only necessary fields**, especially in large nested lists, to ensure the response stays fast.

* **Completeness of commit data:** To fully replace REST, we will include commit fields such as the commit message (full) and verification if available. As noted, adding the commit message body (not just the headline) can be done with a `message` or `messageBody` field in the commit fragment. This is a small addition but ensures parity with the REST commit API which returns the complete commit message. We should also consider including the commit’s tree SHA or verification status if needed (the REST commit JSON includes these). If those are not needed for this tool’s purpose, we won’t add them, to save on data size. The key is to ensure that the `commit_full` objects stored for branches and tags contain all relevant info for later analysis, so that we don’t need any additional API calls. After changes, a branch’s `commit_full` will have commit SHA, author info, dates, message, and CI status – all fetched in one go via GraphQL.

* **Testing performance:** We will test the updated queries against real or mock data to validate performance. This includes using the GitHub API rate limit headers or timing info. For example, we can instrument `_github_graphql` to log the duration of each request (this was already done with `logger.debug` timing). By running the tool on a moderately sized org, we can check that each GraphQL call completes well under 10 seconds. If any single query is approaching the limit, we may adjust our page size downward or split the query. Because the tool logs `API_CALL_COUNT` and total runtime at the end, we can use that to verify that the number of calls stays reasonable (e.g., well under 5000, ideally in the low hundreds for large orgs). In addition, writing unit tests for the GraphQL query builder could catch issues – for instance, we might add a test to ensure the constructed query string for fetching branches after a given cursor contains the expected `after:` parameter and fields. Although we cannot easily measure real API speed in unit tests, we can simulate large responses by creating fake data objects and ensure our code handles them (e.g., extremely large lists) within Python efficiently.

## Development Plan: Step-by-Step Implementation

Finally, here is a proposed sequence of steps to implement the above improvements. Each step is designed to be incremental and verifiable:

1. **Default to GraphQL and Deprecate REST:** Modify the CLI logic to always use GraphQL. Remove the conditional `--graphql` check and have the program call `list_repos_branches_graphql` or `fetch_repos_full_graphql` unconditionally. In this step, we will *not yet delete* the REST functions, but simply stop using them. This way, we can run tests to ensure the GraphQL path produces correct results for all options. We will update `cli()` to require a token (if no token is provided, output an error that authentication is needed, similar to the current parser error for `--graphql` without token). **Validation:** Run the test suite. Adjust tests expecting `list_repos_branches` (REST) to instead expect the GraphQL output. For example, in `test_list_repos_branches_graphql`, ensure it still passes (it should, since we haven’t changed the GraphQL function itself). For `test_list_repos_branches` (REST), we can remove it or rewrite it to test the GraphQL behavior for no repos and error skipping. At this stage, the tool should behave as if `--graphql` is always on.

2. **Remove REST Implementation:** Once GraphQL is the default and tests are green, safely remove the REST-specific code. Delete or comment out functions and blocks that called the REST API: this includes `fetch_repos`, `fetch_branches(_full)`, `fetch_tags_full`, and the entire `else` branch in the CLI that handled the non-GraphQL case. Also remove global constants and variables related to REST (API\_BASE, possibly REQUEST\_TIMEOUT if only used for REST, etc.) and the `_github_get` function. Update any references to those (for instance, tests or documentation). **Validation:** Run tests again to ensure nothing breaks due to these deletions. No test should be referencing removed functions now. This step cleans up the codebase, leaving only the GraphQL logic in place, with a clearer flow.

3. **Refactor and Simplify Logic:** Refactor the remaining code for clarity and modularity. For example, integrate `list_repos_branches_graphql` into `fetch_repos_full_graphql` or vice versa to avoid duplicate loops. One approach is to have `fetch_repos_full_graphql(..., include_details: bool)` where if `include_details=False`, the query omits heavy fields and we only gather names. Alternatively, keep them separate but ensure their internals are DRY (maybe share a common internal function for the main repo pagination loop). Simplify the CLI handling of saving outputs: instead of splitting logic in multiple places, accumulate the results in one data structure and then do file I/O in one section. For instance, after getting all data via GraphQL, create the `branch_map` and `tag_map` (if applicable) and then handle each `--save-...` flag in sequence in one place (this avoids the minor bug with `--save-dir` in listing mode). Also, remove any now-redundant helper like `_apply_gql_page_size` if not used. Ensure the logger usage is consistent (the global logger is configured in CLI; that’s fine). **Validation:** This is mostly an internal reorganization – run the tests to catch any behavioral changes. All persisted files (if tested) should remain the same. You can add a small test for the unified logic: e.g., call the new unified function to get data for a made-up small org (monkeypatching `_github_graphql` to return a couple of repos and branches) and assert the structure of the returned maps is as expected (repo names as keys, etc.).

4. **Enhance GraphQL Query Fields:** Modify the GraphQL query strings to include any missing data and remove extraneous fields as decided. Specifically, add the commit `message` (or `messageBody`) in the commit fragment for branches and tags, so that `commit_full` in output contains the full message. Also add a fragment to handle annotated tags: inside the `tagRefs` query, do something like `target { ... on Commit { ..fields.. } ... on Tag { target { ... on Commit { ..fields.. } } } }`. This will ensure that if a tag object exists, we drill down to its commit. We will also review the repository fields: for instance, if we decide to drop collaborator details or limit topics, adjust those subqueries accordingly. Keep at least the counts (like `collaborators { totalCount }`) if the details are removed, so the information isn’t lost entirely. Reduce the nested depth where possible to lighten the query. **Validation:** Because these changes affect the shape of returned JSON, we must run tests carefully. Update any test fixtures (for example, if a test was checking that a branch dict has only `name` and not `prefix`, or the presence of `commit_full` keys). We should extend tests on the data normalization: e.g., after a fake GraphQL call, ensure that every branch in `branch_map` has a `commit_full` and `commit` with the new fields. If the test suite doesn’t cover it, we add assertions for commit message presence. We should also run an integration test (manually or as an automated script) on a repository with a known annotated tag to confirm that our GraphQL query now captures the tag’s commit (this might be done by creating a dummy repo with an annotated tag in a test environment, or by inspection if real data is available).

5. **Increase Pagination Size and Test Performance:** Adjust the `GRAPHQL_PAGE_SIZE` default to a higher value (e.g. 50). Remove or update the comment that says “default 50 nodes” to reflect the new actual default. With this change, perform a careful test: simulate an organization with more repositories/branches to see how many calls and time it would take. If possible, use a real large organization (or a stress test with repeated fake data) to measure. The goal is that one GraphQL call fetching 50 repos × 50 branches each returns quickly. If we find that in some environments 50 is too high (e.g., the query times out), we might lower to 30 or so. Document this default and recommend that users adjust `GITHUB_GRAPHQL_PAGE_SIZE` env var if needed for their environment. Also, consider implementing logic to dynamically throttle: for example, if a first query times out or fails due to payload, the program could catch that and retry with a smaller page size (though this adds complexity – at minimum we can log a warning if the user’s page size is likely too high). **Validation:** This step mostly affects performance, which unit tests can’t directly measure – but we can assert that the number of calls made is reduced. For instance, create a fake scenario of, say, 120 repos and see that our code calls `_github_graphql` the expected 3 times (with page size 50, 120 repos would be 3 pages: 50+50+20). We can simulate this by having `_github_graphql` return partial data until a certain call count. Ensure that `_github_graphql` retry logic and our pagination still work with the new sizes. Monitor memory usage if extremely large data is fetched – but given Python and the data sizes (JSON of text fields), this should be fine.

6. **Implement Advanced Batching (Optional):** If after the above we are still concerned about hitting the 5000 calls limit for very large orgs, implement the multi-repository branch pagination batching. This is an advanced improvement: detect after the initial repository query which repos need additional branch page fetches, group a few of them, and construct one GraphQL query with multiple `repository(owner:name)` entries (aliased) to fetch their next page of refs simultaneously. Then repeat if further pages remain. We must be careful to not exceed query complexity – grouping too many might slow the query or hit query size limits. A reasonable approach is to batch 5–10 repos’ branch-page fetches in one call. **Validation:** Unit test this by faking a scenario: e.g., two repos have 3 pages of branches each. After the first page, our batching function should request the second pages for both in one `_github_graphql` call, then perhaps the third pages similarly. We can verify via monkeypatch that `_github_graphql` was called with a query containing both repo names. Also verify the combined data is merged into the final `branch_map` correctly. This step should be tested against an actual large repo scenario if possible to ensure it truly saves calls and remains under time limits.

7. **Update Documentation and Examples:** Update the README usage examples to drop `--graphql` and clarify that GraphQL is always used. Note that a token is now required for all operations (since no unauthenticated mode). Document the environment variable for page size tuning and any new performance considerations. Also, if any output format changes were made (e.g., additional commit fields), mention those for users. **Validation:** Documentation changes don’t have automated tests, but we can quickly run the CLI with `-h` to ensure the argparse help reflects removals (the `--graphql` flag should be gone) and that no deprecated options remain.

8. **Final Integration Test:** Run the refactored tool on a real organization (or a sizable test org) to ensure it collects all intended data. Compare a small sample of the output (branch listings, JSON files) with a previous run from the old version or with manual GitHub inspection. This end-to-end validation will confirm that we successfully removed REST calls (we can watch network logs or GitHub API request counts to verify only v4 endpoints are hit) and that performance is acceptable (the run completes without hitting the 10s timeout for any query and uses far fewer API calls than the REST approach).

By following these steps, we will have a maintainable, GraphQL-only ingestion system. The code will be cleaner (only one code path), each function will have a single purpose with clear inputs/outputs, and the GraphQL queries will be optimized to retrieve a wealth of data in minimal calls. This positions the tool to scale to large organizations while respecting API rate limits and timeout constraints, and makes future maintenance (e.g. adding new fields or adjusting queries) much easier. The end result will be a robust utility that efficiently extracts repository, branch, pull request, and commit data across an organization using only the GitHub GraphQL API.
